{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/ashish.palve/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# demonstrate face detection on 5 Celebrity Faces Dataset\n",
    "from os import listdir\n",
    "import os\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(160, 160)):\n",
    "    # load image from file\n",
    "    print(filename)\n",
    "    image = Image.open(filename)\n",
    "    # convert to RGB, if needed\n",
    "    image = image.convert('RGB')\n",
    "    # convert to array\n",
    "    pixels = asarray(image)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    # bug fix\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify folder to plot\n",
    "folder = 'Male_Female_Backup/train/Male/'\n",
    "i = 1\n",
    "# enumerate files\n",
    "for filename in listdir(folder):\n",
    "    # path\n",
    "    path = folder + filename\n",
    "    # get face\n",
    "    print(filename)\n",
    "    face = extract_face(path)\n",
    "    print(i, face.shape)\n",
    "    # plot\n",
    "    pyplot.subplot(42, 7, i)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(face)\n",
    "    i += 1\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "    faces = list()\n",
    "    # enumerate files\n",
    "    for filename in listdir(directory):\n",
    "        # path\n",
    "        path = directory + filename\n",
    "        # get face\n",
    "        face = extract_face(path)\n",
    "        # store\n",
    "        faces.append(face)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "    X, y = list(), list()\n",
    "    # enumerate folders, on per class\n",
    "    for subdir in listdir(directory):\n",
    "        # path\n",
    "        path = directory + subdir + '/'\n",
    "        # skip any files that might be in the dir\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        # load all faces in the subdirectory\n",
    "        faces = load_faces(path)\n",
    "        # create labels\n",
    "        labels = [subdir for _ in range(len(faces))]\n",
    "        # summarize progress\n",
    "        print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "        # store\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return asarray(X), asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male_Female_Backup/train/Female/images39.jpg\n",
      "WARNING:tensorflow:From /home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Male_Female_Backup/train/Female/images35.jpg\n",
      "Male_Female_Backup/train/Female/images46.jpg\n",
      "Male_Female_Backup/train/Female/image7.jpeg\n",
      "Male_Female_Backup/train/Female/images16.jpg\n",
      "Male_Female_Backup/train/Female/images40.jpg\n",
      "Male_Female_Backup/train/Female/images8.jpg\n",
      "Male_Female_Backup/train/Female/image24.jpeg\n",
      "Male_Female_Backup/train/Female/images4.jpg\n",
      "Male_Female_Backup/train/Female/images19.jpg\n",
      "Male_Female_Backup/train/Female/image27.jpeg\n",
      "Male_Female_Backup/train/Female/images17.jpg\n",
      "Male_Female_Backup/train/Female/images10.jpg\n",
      "Male_Female_Backup/train/Female/image14.jpeg\n",
      "Male_Female_Backup/train/Female/image16.jpeg\n",
      "Male_Female_Backup/train/Female/images13.jpg\n",
      "Male_Female_Backup/train/Female/images43.jpg\n",
      "Male_Female_Backup/train/Female/images.jpg\n",
      "Male_Female_Backup/train/Female/images11.jpg\n",
      "Male_Female_Backup/train/Female/images5.jpg\n",
      "Male_Female_Backup/train/Female/images27.jpg\n",
      "Male_Female_Backup/train/Female/image11.jpeg\n",
      "Male_Female_Backup/train/Female/images44.jpg\n",
      "Male_Female_Backup/train/Female/images33.jpg\n",
      "Male_Female_Backup/train/Female/image28.jpeg\n",
      "Male_Female_Backup/train/Female/image29.jpeg\n",
      "Male_Female_Backup/train/Female/images22.jpg\n",
      "Male_Female_Backup/train/Female/image13.jpeg\n",
      "Male_Female_Backup/train/Female/images1.jpg\n",
      "Male_Female_Backup/train/Female/images6.jpg\n",
      "Male_Female_Backup/train/Female/images21.jpg\n",
      "Male_Female_Backup/train/Female/images15.jpg\n",
      "Male_Female_Backup/train/Female/images34.jpg\n",
      "Male_Female_Backup/train/Female/images36.jpg\n",
      "Male_Female_Backup/train/Female/image17.jpeg\n",
      "Male_Female_Backup/train/Female/images29.jpg\n",
      "Male_Female_Backup/train/Female/images14.jpg\n",
      "Male_Female_Backup/train/Female/images42.jpg\n",
      "Male_Female_Backup/train/Female/images18.jpg\n",
      "Male_Female_Backup/train/Female/images30.jpg\n",
      "Male_Female_Backup/train/Female/image21.jpeg\n",
      "Male_Female_Backup/train/Female/image20.jpeg\n",
      "Male_Female_Backup/train/Female/image12.jpeg\n",
      "Male_Female_Backup/train/Female/images2.jpg\n",
      "Male_Female_Backup/train/Female/images3.jpg\n",
      "Male_Female_Backup/train/Female/image19.jpeg\n",
      "Male_Female_Backup/train/Female/images9.jpg\n",
      "Male_Female_Backup/train/Female/images7.jpg\n",
      "Male_Female_Backup/train/Female/images37.jpg\n",
      "Male_Female_Backup/train/Female/image10.jpeg\n",
      ">loaded 50 examples for class: Female\n",
      "Male_Female_Backup/train/Male/images179.jpg\n",
      "Male_Female_Backup/train/Male/images134.jpg\n",
      "Male_Female_Backup/train/Male/images35.jpg\n",
      "Male_Female_Backup/train/Male/images64.jpg\n",
      "Male_Female_Backup/train/Male/images16.jpg\n",
      "Male_Female_Backup/train/Male/images153.jpg\n",
      "Male_Female_Backup/train/Male/images205.jpg\n",
      "Male_Female_Backup/train/Male/image23.jpeg\n",
      "Male_Female_Backup/train/Male/images117.jpg\n",
      "Male_Female_Backup/train/Male/images40.jpg\n",
      "Male_Female_Backup/train/Male/images139.jpg\n",
      "Male_Female_Backup/train/Male/images52.jpg\n",
      "Male_Female_Backup/train/Male/images65.jpg\n",
      "Male_Female_Backup/train/Male/image24.jpeg\n",
      "Male_Female_Backup/train/Male/images189.jpg\n",
      "Male_Female_Backup/train/Male/images4.jpg\n",
      "Male_Female_Backup/train/Male/images211.jpg\n",
      "Male_Female_Backup/train/Male/images137.jpg\n",
      "Male_Female_Backup/train/Male/images204.jpg\n",
      "Male_Female_Backup/train/Male/images74.jpg\n",
      "Male_Female_Backup/train/Male/images129.jpg\n",
      "Male_Female_Backup/train/Male/image14.jpeg\n",
      "Male_Female_Backup/train/Male/images20.jpg\n",
      "Male_Female_Backup/train/Male/images84.jpg\n",
      "Male_Female_Backup/train/Male/images.jpg\n",
      "Male_Female_Backup/train/Male/images63.jpg\n",
      "Male_Female_Backup/train/Male/images163.jpg\n",
      "Male_Female_Backup/train/Male/image15.jpeg\n",
      "Male_Female_Backup/train/Male/images159.jpg\n",
      "Male_Female_Backup/train/Male/images199.jpg\n",
      "Male_Female_Backup/train/Male/images172.jpg\n",
      "Male_Female_Backup/train/Male/images59.jpg\n",
      "Male_Female_Backup/train/Male/images180.jpg\n",
      "Male_Female_Backup/train/Male/images48.jpg\n",
      "Male_Female_Backup/train/Male/image13.jpeg\n",
      "Male_Female_Backup/train/Male/images1.jpg\n",
      "Male_Female_Backup/train/Male/images187.jpg\n",
      "Male_Female_Backup/train/Male/images15.jpg\n",
      "Male_Female_Backup/train/Male/images173.jpg\n",
      "Male_Female_Backup/train/Male/images58.jpg\n",
      "Male_Female_Backup/train/Male/images56.jpg\n",
      "Male_Female_Backup/train/Male/images200.jpg\n",
      "Male_Female_Backup/train/Male/images61.jpg\n",
      "Male_Female_Backup/train/Male/images123.jpg\n",
      "Male_Female_Backup/train/Male/images23.jpg\n",
      "Male_Female_Backup/train/Male/images116.jpg\n",
      "Male_Female_Backup/train/Male/images9.jpg\n",
      "Male_Female_Backup/train/Male/images7.jpg\n",
      "Male_Female_Backup/train/Male/images12.jpg\n",
      "Male_Female_Backup/train/Male/images91.jpg\n",
      ">loaded 50 examples for class: Male\n",
      "(100, 160, 160, 3) (100,)\n",
      "Male_Female_Backup/val/Female/2.jpg\n",
      "Male_Female_Backup/val/Female/farhana-maqsood-1-300x300.jpg\n",
      "Male_Female_Backup/val/Female/2ed89-15450721145209-500.jpg\n",
      "Male_Female_Backup/val/Female/3148_natalie_732x549-thumb.jpg\n",
      "Male_Female_Backup/val/Female/11.jpg\n",
      "Male_Female_Backup/val/Female/female-punjab-singer.jpg\n",
      "Male_Female_Backup/val/Female/302035_1100.jpg\n",
      "Male_Female_Backup/val/Female/female-blushing.jpg\n",
      "Male_Female_Backup/val/Female/326403_1100.jpg\n",
      "Male_Female_Backup/val/Female/06-Jessica-biel.jpg\n",
      "Male_Female_Backup/val/Female/alana-blanchard-mobile-wallpaper.jpg\n",
      "Male_Female_Backup/val/Female/19691652-7571245-image-a-2_1571055758081.jpg\n",
      "Male_Female_Backup/val/Female/91651699.jpeg\n",
      "Male_Female_Backup/val/Female/200116231238-san-francisco-giants-female-coach-trnd-exlarge-169.jpg\n",
      "Male_Female_Backup/val/Female/321032_1100.jpg\n",
      ">loaded 15 examples for class: Female\n",
      "Male_Female_Backup/val/Male/40-pisuetlpp71677-peter-england-original-imafmzubesvnyryf.jpeg\n",
      "Male_Female_Backup/val/Male/6-skincare-tips-men-should-always-follow-peter-kraus-00-722x406.jpg\n",
      "Male_Female_Backup/val/Male/best-mens-medium-length-hairstyles2.jpg\n",
      "Male_Female_Backup/val/Male/447932206319ff09a88fcf8cc96119cf--male-senior-pictures-senior-photos.jpg\n",
      "Male_Female_Backup/val/Male/1189-Male_Sexual_Performance-732x549-thumbnaill.jpg\n",
      "Male_Female_Backup/val/Male/81-KQKLZvQL._AC._SR360460.jpg\n",
      "Male_Female_Backup/val/Male/ff371ff9-b0a5-4672-bf25-887fd64c35441566371512914-HM-Men-White-Solid-Easy-iron-shirt-Slim-Fit-6231566371512073-1.jpg\n",
      "Male_Female_Backup/val/Male/99253-96654.jpg\n",
      "Male_Female_Backup/val/Male/f802fa14-5781-4f42-b11b-f90ff42fcc7b.jpg\n",
      "Male_Female_Backup/val/Male/2cd0643b-0e45-46d8-919c-0761570074141580464871899-Emerging-Brands_JAINISH_Beevee-and-Fido-Dido_DMPB_B2.jpg\n",
      "Male_Female_Backup/val/Male/adult-business-businessman-428339.jpg\n",
      "Male_Female_Backup/val/Male/a-young-man-on-a-laptop-in-bed.jpg\n",
      "Male_Female_Backup/val/Male/271647_1100.jpg\n",
      "Male_Female_Backup/val/Male/266749_1100.jpg\n",
      "Male_Female_Backup/val/Male/001ec97909631412122b07.jpg\n",
      ">loaded 15 examples for class: Male\n",
      "(30, 160, 160, 3) (30,)\n"
     ]
    }
   ],
   "source": [
    "# load train dataset\n",
    "trainX, trainy = load_dataset('Male_Female_Backup/train/')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "testX, testy = load_dataset('Male_Female_Backup/val/')\n",
    "print(testX.shape, testy.shape)\n",
    "# save arrays to one file in compressed format\n",
    "np.savez_compressed('Male_Female_Backup-dataset.npz', trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (100, 160, 160, 3) (100,) (30, 160, 160, 3) (30,)\n"
     ]
    }
   ],
   "source": [
    "# load the face dataset\n",
    "data = np.load('Male_Female_Backup-dataset.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the keras facenet model\n",
    "from keras.models import load_model\n",
    "# load the model\n",
    "#model = load_model('facenet_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = load_model('facenet_keras.h5')\n",
    "print('Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a face embedding for each face in the dataset using facenet\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the face embedding for one face\n",
    "def get_embedding(model, face_pixels):\n",
    "\t# scale pixel values\n",
    "\tface_pixels = face_pixels.astype('float32')\n",
    "\t# standardize pixel values across channels (global)\n",
    "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
    "\tface_pixels = (face_pixels - mean) / std\n",
    "\t# transform face into one sample\n",
    "\tsamples = expand_dims(face_pixels, axis=0)\n",
    "\t# make prediction to get embedding\n",
    "\tyhat = model.predict(samples)\n",
    "\treturn yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (100, 160, 160, 3) (100,) (30, 160, 160, 3) (30,)\n",
      "Loaded Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/ashish.palve/.local/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "data = load('Male_Female_Backup-dataset.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "# load the facenet model\n",
    "model = load_model('facenet_keras.h5')\n",
    "print('Loaded Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 128)\n"
     ]
    }
   ],
   "source": [
    "# convert each face in the train set to an embedding\n",
    "newTrainX = list()\n",
    "for face_pixels in trainX:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tnewTrainX.append(embedding)\n",
    "newTrainX = asarray(newTrainX)\n",
    "print(newTrainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 128)\n"
     ]
    }
   ],
   "source": [
    "# convert each face in the test set to an embedding\n",
    "newTestX = list()\n",
    "for face_pixels in testX:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tnewTestX.append(embedding)\n",
    "newTestX = asarray(newTestX)\n",
    "print(newTestX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save arrays to one file in compressed format\n",
    "savez_compressed('Male_Female_Backup-embeddings.npz', newTrainX, trainy, newTestX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop a classifier for the 5 Celebrity Faces Dataset\n",
    "from numpy import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train=100, test=30\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = load('Male_Female_Backup-embeddings.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize input vectors\n",
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(trainX)\n",
    "testX = in_encoder.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(trainy)\n",
    "trainy = out_encoder.transform(trainy)\n",
    "testy = out_encoder.transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "yhat_train = model.predict(trainX)\n",
    "yhat_test = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: train=92.000, test=80.000\n"
     ]
    }
   ],
   "source": [
    "# score\n",
    "score_train = accuracy_score(trainy, yhat_train)\n",
    "score_test = accuracy_score(testy, yhat_test)\n",
    "# summarize\n",
    "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop a classifier for the 5 Celebrity Faces Dataset\n",
    "from random import choice\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load faces\n",
    "data = load('Male_Female_Backup-dataset.npz')\n",
    "testX_faces = data['arr_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load face embeddings\n",
    "data = load('Male_Female_Backup-embeddings.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize input vectors\n",
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(trainX)\n",
    "testX = in_encoder.transform(testX)\n",
    "# label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(trainy)\n",
    "trainy = out_encoder.transform(trainy)\n",
    "testy = out_encoder.transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/ashish.palve/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# test model on a random example from the test dataset\n",
    "selection = choice([i for i in range(testX.shape[0])])\n",
    "random_face_pixels = testX_faces[selection]\n",
    "random_face_emb = testX[selection]\n",
    "random_face_class = testy[selection]\n",
    "random_face_name = out_encoder.inverse_transform([random_face_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Male (94.271)\n",
      "Expected: Male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/ashish.palve/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# prediction for the face\n",
    "samples = expand_dims(random_face_emb, axis=0)\n",
    "yhat_class = model.predict(samples)\n",
    "yhat_prob = model.predict_proba(samples)\n",
    "# get name\n",
    "class_index = yhat_class[0]\n",
    "class_probability = yhat_prob[0,class_index] * 100\n",
    "predict_names = out_encoder.inverse_transform(yhat_class)\n",
    "print('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\n",
    "print('Expected: %s' % random_face_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAEICAYAAABWCOFPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnWm0ZFd1mL9d06s3v349qSepBzUSAjMoilCwsTEygwSRIAFbhIUUrCxlwDbEIZYwaxmc2A4Em8GxgxY2BAnLyEKAkQkEFA1mxbEEktDcamgJIfXcUk+v31Djzo+61arb3LNP9Ruqq539rfXWqzrnnuGee2vfc/c+Z29RVRzHcdrkTnUHHMfpL1woOI6TwoWC4zgpXCg4jpPChYLjOClcKDiOk8KFwj9QRGSjiKiIFOZZ/jwRuW+x+7WUiMhqEdkmIgOnui+nMy4U+hAReVpEqiKy4oT0B5Mf+sYedOM/A3/Y0faLReROETkiIjtE5G1ZhUTkw0kffymQv0pEviQiu5O6/k5EXtWR/9sicqzjb1ZEmu2xEJFfFpH/KyIzInJ3Z92qug+4C7hm4af//y8uFPqXHwPvbH8RkZ8BBnvRsIisAX4R+OvkewH4OvANYJLWj+4vRORFJ5TbArwd2GNUPwJ8H/hHSV03AP9TREYAVPUPVHWk/Qd8DLhbVZ9Lyh8EPgV8NFD/TcC/PrkzdjpxodC/fBG4suP7VcCNnQeIyJtF5AciclREnhWRj4QqE5FxEfmciOwRkV0i8nsikg8c/nrgAVWdS76fC6wFPqmqDVW9E/g74N0nlPsT4FqgGuqHqj6lqp9Q1T1JXZ8FSsA5GX2WpI0bOsr/b1W9BdgdaOJeYLOInBXqg2PjQqF/uQcYS6bteeBXgL844ZhpWoJjAngz8G9F5K2B+m4A6sDZwCuBNwD/KnDszwDbO75LxjECvPT4F5F3AFVV/aZ1Uj9VicgraAmFHRnZrwFWA1/ptj5VrSd1vfxk+uG8gAuF/qY9W3g98ASwqzNTVe9W1UdUtamqDwNfAn7hxEpEZDVwCfB+VZ1W1f3AJ4ErAu1OAFMd358A9gP/UUSKIvKGpJ2hpP4R4A+A95/MyYnIWHKOv6uqRzIOuQq4VVWPnUy9Sd8nTrKMkzAvzbTTM74IfBfYxAmvDgCJgu6jtJ7YJWAA+HJGPWcBRWBPa0YOtB4IzwbaPQSMtr+oai2Zgfw3Wq8H9wG3AJXkkN8FvqiqP+72xERkEPgb4B5V/S+B/HcAl3dbZwejwOF5lHPwmUJfo6o/oaVwvBT4asYhfwncBmxQ1XHgerKn+s/S+gGvUNWJ5G9MVV8SaPphIKVEVNWHVfUXVHW5qr4R2Ax8L8m+GPgNEdkrInuBDcAtInJtVuWJyfCvac18QkrBf0ZLqXh3ID+TRCl6NvDQyZRzXsCFQv9zNfA6VZ3OyBsFDqrqnIhcCPyLrApUdQ/wHeCPRGRMRHIiskVEfupVI+F24HwRKbcTRORlIlIWkSER+QCwBvhCkn0xrdnKK5K/3bR+7H96YsUiUgRuBWaBK1W1GejDVcCNesLefhHJJ/0qALmkT8WOQy4Enk4EqjMPXCj0Oar6pKqGFhH9O+A/icgU8Du0pvQhrqT1ivE4rdeDW2n9sLPa3AfcSXrq/m5apsb9tITA61W1khz/vKrubf8BDeBQWxcgIteLyPVJPa8G3kJL0Xm4Yz3Ca9oNicg64HVkvDIl/ZgFPkNLETkL/FlH/rtozZiceSLuZMXJQkTOo2WxuPDEp3W/IiKrgL8FXtlhTnVOEhcKjuOk8NcHx3FSLJlQEJE3icj2ZJ38dUvVjuM4i8uSvD4kK/B+SGvRzU5aa93fqaqPL3pjjuMsKku1eOlCYIeqPgUgIjfT0mRnCoV8TrSYC09axBBckmWVP4n8nNHu8WPy2cdII2RNax8QF7ih9vO50LaEzrKRk4tgPxDsuvOBMTle2hj42Jjn8/a5d3PNCvnsW7tQsG95JX7NQmfWbMbLHp06aubPzMya+WqMayNyzWaqjedUdaV5EEsnFNaRXi23E3hV5wEicg3JFtdCTtg4PhysLFcPK5IHCvYNUiia2ZTLZfsAYGRsNDO9OD2Vmd5GcvGbZHAwe+Pj6Gh2m50MDcT6bgutRqMRzLN+1ABjkf5ZP77h4fC1BpiYGDfzu7lmk5OTmekrVqzITG9jjUmbkFCai/ygAe64604z/74HHzbza4ariGORn/P3njnY1dqNpdIpZN1RqV+Iqn5WVS9Q1Qvysce54zg9Y6mEwk5aS13brCe81dVxnD5iqYTC94GtIrJJREq0duPdtkRtOY6ziCyJTkFV6yLya8C3gTzweVV9LNiJfI6JkaFgfcOF8Dvk+JCtNBgest31lSP5EH6/X15Ya5YrlUrRukPHxBRiAAMDdt9jeonx8fC7e+y9P6bsGxoKX8+RkRGz7HB54Q6mQnqHQkQH9fzzz0frfuqppzLT9x7YHy1bnbH1UGtWLjfz5wgrYWW6Hm2/G5Zs63TibOOkHG44jnPq8RWNjuOkcKHgOE4KFwqO46RwoeA4TgoXCo7jpOgLx62FfJ5Vy8eC+WuXhc1b61YtM+teucxeMjs0HF8yWwqY/iYia/RjZj0Im+cKxbipNGaSjC0Htkymzcg6+tDy7DaWybJUsMdt0DBnAtDFJr5qNTv0xOysvRS5UYub9dYGzIZP/vCJaFnrPgcYHbPvmanZ8NL10br9jL9juxWj5wV8puA4TgoXCo7jpHCh4DhOChcKjuOkcKHgOE6KvrA+5PPCxHBYE75mRdiCsG6Frc1ducIOKTgxapcHyBWzh2nFqG356GZDVLGYvaErF9HQAxTydv2huttYFoZ63dbCx7wjmfmRzVQasXxIpO1WG7VA03bb3ViMdu/fmZm+fu2qaNlV6+y4t/sPZoXUfIGHH8uKw9vime1PR9vvBp8pOI6TwoWC4zgpXCg4jpPChYLjOCn6QtFYzBdYvTystLO80UyM2Et9x8q2Mm48Uh4MhWHExXpxIK5oDC5zjigRAepN2/Nws2l7c7a6Xyzb4zJXy15G3Ga0HPb6FFP2VRuRpcZxh8vUA+7WNeLRqhhbYg1s3bwlM33jlo3RslMVe5l1JTKuL9p8VjBv1/6D0fa7wWcKjuOkmLdQEJENInKXiGwTkcdE5H1J+qSI3C4iP0r+23Y7x3H6ioXMFOrAf1DVFwMXAe9NwpdfB9yhqluBO5LvjuOcJsxbKKjqHlV9IPk8BWyjFRnqcuCG5LAbgLcutJOO4/SORdEpiMhG4JXAvcBqVd0DLcEBZC7zEpFrROQ+Eblvppq9+sxxnN6zYKEgIiPAV4D3q6odPbODzrBxQ6VIwEfHcXrGgkySIlKkJRBuUtWvJsn7RGSNqu4RkTVANELGQLnE2VvCppZVI2GhUcQ24QwP2qa1Ri0+SxkOBE1p5iNBS7qICt0MRDluStzuFg1gqxGTpLGHIBeJ71mMCPJmM2xWLJVsj1C1mn3u9S6CwIYC5NYiezqQ+L6KkCW43kWU8UbkmsT2XmxYF85/0cZDkdYfjeS3WIj1QYDPAdtU9RMdWbcBVyWfrwK+Pt82HMfpPQuZKfws8G7gERF5MEn7beCjwC0icjXwDPCOhXXRcZxeMm+hoKr/h+yQ8wAXz7dex3FOLb6i0XGcFH2x9yGfyzE6HFba5XKGcqhuK3di6/9jrsoBqvXsOgZKtlIq14WisVqtZKbXAy7KO8kX7PpjewzUGJu6xts3qYXbbtazz7mN5O3bUjU+rrVGSIFrl8vl43XnC9nK69i9BlCPOJApxBzzGErWWDiDbvGZguM4KVwoOI6TwoWC4zgpXCg4jpPChYLjOCn6wvogQD6yrDZEPuB+vY1Glq1KMR5gtqnZdVQiS6S7sWzk89lyudCFFrwYKNumVLDzLW25RoK45qMWgnD5JraWvhix6jS0i2dZPVtLb6y+BuIBaAGK+ex7ZrYaLztdtZdoa8Mem4FceGw2bTgz2n43+EzBcZwULhQcx0nhQsFxnBQuFBzHSeFCwXGcFC4UHMdJ0RcmSVU1oxzXCmEzTmnA9qzUFPsUK/W46S8X2KTSxDadNQ3zUZtGoPl8F/K6FjGJSsR0J0ZUlcFSJBjMnG1+k3zYM1NTbLNbtWJvmIpY9VoENi2VBmwzcTd1T09lj3sjFw8sVBwJB8kBmDkUCehijM1QOR4xuxt8puA4TgoXCo7jpHCh4DhOisVw8Z4XkR+IyDeS75tE5N4kbNxfiUg8UqrjOH3DYiga30crOtRY8v1jwCdV9WYRuR64GviMVUEulzNdW9crU8G86Rlb4TVbtdfwT+08YOYDFErZyqnRsbHM9DZjo3HFz/hodtTp1SviITjLkf0REslv1MIKv7mqvUmgEHHTbu0L0UhE7VzOdh9fj+wPADhydCYz/dDhvWa5XXv3Res+tvtwZvrh5rFoWS3Zfa9P24rGrSvWBvO2nHl2tP1uWNBMQUTWA28G/jz5LsDrgFuTQzxsnOOcZiz09eFTwG/B8W1vy4HDqtp+zOykFV/yp+gMG3d0Or67zHGc3rCQYDBvAfar6v2dyRmHZs7fO8PGjRlOWx3H6S0LDQZzmYhcCpRp6RQ+BUyISCGZLawHdi+8m47j9IqFhKL/oKquV9WNwBXAnar6LuAu4O3JYR42znFOM5ZimfO1wM0i8nvAD2jFmzRpNJtMTYc1t9sfeySYt3fPHrPuXNF+NTk2F3HFg+HnvzRklhspx5e9rlyWbcF4yTlbomXXrrQtFFvOzFTnHGeoFL789UgMg0ZkKfL0zFwwrzhoW0UOT0+b+Q9v+5GZD7Dtyacy05/ZZVsXpiLWLIDVxeylygfrR6Jlq2LXP4i9dL18bngd9uqJldH2u2FRhIKq3g3cnXx+CrhwMep1HKf3+IpGx3FSuFBwHCeFCwXHcVK4UHAcJ0VfOFnJR/Y+rF4b1qI3IlF8j83a1oXZZlzbfOhotib9UNXWFFciWnSARjXb6vLgo9uiZdevXm7mv/G1P2fmn7vlrGDe0KBtOalErA8VY+9EQe1rcs+Dj5n53/j2HWY+wLN7n8tMl4K9Z2PFGeG9BW2m57ItAOu3vihadt2WVWb+1jMmzfwNhsVrqLA4ew99puA4TgoXCo7jpHCh4DhOChcKjuOkcKHgOE6KvrA+KNBshj0knXHGGcG88QlbW1tt2nIvX7JdbgMcnsq2UOzYb1sXDh3YH637wN5nM9Or9bhVZN+BbA9AbfYftNfir14VbmN6zrYujI7a43bw6NFg3pG9h8yyjz6+3a77SLZXpU4mV6/PTK81bbf7MU9dAKXRbAuAdOHS/+Bh+5pUlo+b+TOGtaxUXpxnvM8UHMdJ4ULBcZwULhQcx0nhQsFxnBR9oWjMqTJUDzuPqBvKwkZErk0dsV1m3//QnXbngOlA3MQfHrIDDzZq1WjdxUb2MauXxRWgWzZsMPMrh22HIpWD4TYmlttLqEfq9q2zeiisdNv1o2wHKG2ee9pWNC4vx5WBv3hxtkuPL3/9b8xycxV76TrAwMSazPTpn+yKlh2Ztse1OWQvVS5syFagAuTy8XHpBp8pOI6TwoWC4zgpFhoMZkJEbhWRJ0Rkm4j8ExGZFJHbk7Bxt4tIPNSR4zh9w0JnCp8G/peqngu8nFb4uOuAO1R1K3BH8t1xnNOEhQSDGQN+nsRbs6pWVfUwcDmtcHHgYeMc57RjITOFzcAB4H8kUaf/XESGgdWqugcg+W97lXAcp69YiEmyAJwP/Lqq3isin+YkXhVE5BrgGoBV4yMUCuGuiIZNLTOH7f0He/faUYbzEpeLLzn3xZnphWfD6/sBtm+zPQgB/PLbLstMn42YUgGeedI23W0Yt6NiTxnr8F+8ZZNZdtkyW1VU3xPe9zGQt2+7V5yXPd5tdu+Lj814IHL15a+92Cx34Hl7XwbAy8/N9s5Ur8djiIwO2p6fVq2YMPMHB8NxTCR/6vc+7AR2quq9yfdbaQmJfSKyBiD5n3l3dMaSHPdYko7TNywkbNxe4FkROSdJuhh4HLiNVrg48LBxjnPasdAVjb8O3CQiJeAp4D20BM0tInI18AzwjgW24ThOD1mQUFDVB4ELMrLsFzfHcfoWX9HoOE6KvtgQJSLk8+ENNEXDMrFuffbmlDbjgajObfY/d8DuHDA0lO1pZ8PyzWa5t77m1dG6i5JtWZkezNaed3L2Stvr1PMH7Ijchw+FtfjVqr2Zqx6JeaHNsCZ+YiQc4wOgsC686Qdg6rl4dOd9O3Zkpq85w95EtmpNPO7DYCk7JkZ5zPaaBLBiuW21mZwYMfOlEY4GXo9cs27xmYLjOClcKDiOk8KFguM4KVwoOI6Toi8UjQBSMALFSli5MhZQArZZudL2dLN80l5WCmGlW2nGduldq8W9+OQCYnnZyhXRsocO2UrSWsAVeZs1a1YH8+p1W2klEW/mxYGwB6E1Z4TbBZibtcetm0CqGnDlXijYCtzJZfFxz63IXqpcHoz3a3TIXr1bLEVc0BtBi+tN2xNYt/hMwXGcFC4UHMdJ4ULBcZwULhQcx0nhQsFxnBQuFBzHSdEXJklFqTfC5pTyQNiMVCzYcq2UM0ydwMplcZNksZjd/mDNbntmJh4deXAoex390WnbqxMAedtsODRqm8gazbCpVyNefI4FAuR0U35ozA50UyzZ5xXz+gSQI9u0Nztr93siEsUcIDeRvXejELkXW4THHKDZsM2xtUZ4T4mbJB3HWRJcKDiOk8KFguM4KVwoOI6Toj8UjarU6pVg/ojh7XlsxF7fXy7bLrW7cfHeCChBNWcrjQYH4l6qQ/sjikNxJyvDDdtZCbb3++B5AVSMKOAAoxE37QNG/5uErzXAQMk+d63b424xuSwS9dlQvh4n4Mo9n89WGqcIbXZJKJRs5XDI4Q9Arrg4P+eFxpL89yLymIg8KiJfEpGyiGwSkXuTWJJ/lTh1dRznNGEhYePWAb8BXKCqLwXywBXAx4BPJrEkDwFXL0ZHHcfpDQvVKRSAQREpAEPAHuB1tALDgMeSdJzTjoUEg9kF/CGt2A57gCPA/cBhVW2/dO0E1mWVF5FrROQ+EbnvyPTcfLvhOM4is5DXh2W0IkxvAtYCw8AlGYdmuitOh42zlYGO4/SOhagrfwn4saoeABCRrwKvBiZEpJDMFtYDu6M1iSCGi/eqYZmYNjzRQFzqDZXjFoL6XPZMRofs2kPLo1PtB6wns9PhoLrH+3XEthBY1gWAZi2saR8ZtF2ND5bscatU5n/NSmVbi18wlr23yQcsGMVh21p1LNI3gMmAVSmft5fUA0xHlodXGrZlZqYSLl/T+VtlOlmITuEZ4CIRGRIR4YVYkncBb0+O8ViSjnOasRCdwr20FIoPAI8kdX0WuBb4TRHZASwHPrcI/XQcp0csNJbkh4EPn5D8FHDhQup1HOfU4cucHcdJ0RfLnEXEdAluLT2t1ey999qwFWIDxYivciAXUHxVBiPKwLiukNlKts+FbpSUNO0GtGornibHwr4kSmq3n6/bz5PBQnjcx4fsmIt1ifgFkLhCTzV7bI7N2YrE0Qnb1wPAQD1wr+biF1wCsUPbxJZZWz4wpCt/DnF8puA4TgoXCo7jpHCh4DhOChcKjuOkcKHgOE4KFwqO46ToC5MkAIYr9lotbKKqB7zgtKlU7bXmc3NxHzADxexjGthtFyMRjgGamm0Src/ZdbeOsU13w2V7/4I2ws8EUfvWKObtTWzThnv7ctEuOxu5ZkePTJn5AIFhZSgSiVuH4m7SK4H9B9V6PMp4pWbvbWiq3X7I1AqYYRJOBp8pOI6TwoWC4zgpXCg4jpPChYLjOCn6QtEoIuSNtf5qKHBqkfh5MeVPtWrvnQDIBdxya0QZWBiKe5SarWT3b++e/dGyc3N230PxFNsUC+H8o9N2HEwV+9axPOfH4mTGnJXEHLwATAf2ODRn7WumQ3FlYbOZ3b9mM64cju1taEYcpdSN/Eaji802XeAzBcdxUrhQcBwnhQsFx3FSuFBwHCdFVCiIyOdFZL+IPNqRNikityeh4W5P3L0jLf5YRHaIyMMicv5Sdt5xnMWnG+vDF4A/AW7sSLsOuENVPyoi1yXfr6UV92Fr8vcq4DPJf5NcLsfgYFijfKwSDhZjeaIBqFRtjfBs1V52CuHAnXlsLXnMOgAwPZN9bse6CJATq/3A/ufM/Lm58LkfiLlhjwSYrdbC/X/22WfMsps3nWXmX3ThPzbzAcYns7071SPXe/aobXUByA2GrA9xF+uNyDLmRsRdl7XMudbFMutuiM4UVPW7wMETki+nFRIO0qHhLgdu1Bb30IoBsWZReuo4Tk+Yr05htaruAUj+r0rS1wHPdhzXVdi4Q8fi0tlxnN6w2IrGrHlVNGzcskCUJMdxes98hcK+9mtB8r+9/G4nsKHjuO7CxjmO0zfMVyjcRiskHKRDw90GXJlYIS4CjrRfMxzHOT2IWh9E5EvAa4EVIrKTVkSojwK3iMjVtGJKviM5/JvApcAOYAZ4zxL02XGcJSQqFFT1nYGsizOOVeC9J9sJyeUoDw0H848dDW+gqUfMMNYGEoC5SDAZgFIjEAU54n1orho3EVUDZqy8YaJtY1inAKhENhZNNcLm2nzFNknGzLErV64M5p2z/GVm2Q1rbINVedz2KAXQCEQKL0QcbQ0Oh+/DNpWAGTlmTgTbpAjxDVGmSTKyObBbfEWj4zgpXCg4jpPChYLjOClcKDiOk6I/PC/lcpRKYaVd05BdqrbCqxZR7MzW4t5yCgGF4fCgvehKcvHhzZWzvR/lSl2sY4/srVizPnMx6XFWGK7zCwW77+PjduTowVJAOQsMDYbzAAZKtjbwiKF4bpMLRXeOeEfqxk17IxBWQCMRpQEaMc9KETftlqJR665odBxnCXCh4DhOChcKjuOkcKHgOE4KFwqO46ToD+uDCMUBWyMdImJ8oBmzPnQR9yEf8NZTmzpmt92FNjhkPYmcFgCjA/ZS6OEhezlw3ojdEPOsVDTidADUG+FxHTEsTQCSs69ZJRKTAqBQCDzvxNb+d+O7qFHLPiof8NCVqj+yFLkWqLtNswePcZ8pOI6TwoWC4zgpXCg4jpPChYLjOClcKDiOk6IvrA+5XI5yObyPIFcIa7obkbXiMU/8tS7iPhSr2WvxC6XIvgtjb0GbfEidHIhG3cnkQMS6EKlC6+FzD2zJOE6jMWvmi3FdGnP2uAxP2OelgajPnUwfC0SdJqL9NxzPtCkGrAzlXLxfsdgQsfuZXPjC5PORi9YlPlNwHCfFfMPGfVxEnkhCw31NRCY68j6YhI3bLiJvXKqOO46zNHQzU/gC8KYT0m4HXqqqLwN+CHwQQETOA64AXpKU+e8isjhzGsdxesK8wsap6ndUtf3ydQ+t+A7QCht3s6pWVPXHtLw6X7iI/XUcZ4lZDJ3CrwLfSj7PK2zcwcNTi9ANx3EWgwVZH0TkQ0AduKmdlHFYMGwc8FmAl5+7WUuGtx3LC1CjZsu1WJTfehdusUPHTMU8ADXjnnimD2bX8eTj26NlJ8qjZn4tEkF5+ki4/5VIfM+Bou0dSfLh61Jp2maRsRUTZn69EB/XOtlWhI3nbDHLnbH2jGjdMpxtKevGxXuMptrWD2u/Sj6yX6Vb5l2LiFwFvAW4WF/wEeVh4xznNGderw8i8ibgWuAyVe18pNwGXCEiAyKyCdgKfG/h3XQcp1fMN2zcB4EB4HYRAbhHVf+Nqj4mIrcAj9N6rXivamT+7jhOXzHfsHGfM47/feD3F9Ipx3FOHX2xzBlt0GwcCWafvXl5MO/++35iVj08usLMrzRsZR3ArGYfs6x+MDO9TS7XxdtZ4AosH4vHNNy/84Ddft1edjs5ujqYN3HGMrPs4IDt3n5kxFiqHLnrpht2HMvBSdtJC8DqM1dlphdHbecwx+ZsxzkAxUa2tWxupovYoTl7CXe1OGbmq+GEZXw4Hn+0G3yZs+M4KVwoOI6TwoWC4zgpXCg4jpPChYLjOClcKDiOk6I/TJIi5IwIzQUjuvO6M88yq961c7+ZPzQUNne2OXw021w6VLRNUBORyMwA42PZpr9YVGeAtWvDZlyA2cO2d6TmbHhdmVZtD0FHZ21zbFXDeyeGJ2wz8OQqe+/DyErbrAdQGsq+n5o5+7zKg/aeDoDGXLZZc24u7rWpkrM9fTWMvQ0ANMPxNMbGbPN7t/hMwXGcFC4UHMdJ4ULBcZwULhQcx0nhQsFxnBR9YX1QVSqGv/vBcjgi9cpVtqec3bvsTUONSjyCcTkQBflYxdbu52biw9sczK67GYm8DFAcsetvRi5vJRce82okaMTgqL0pKS/h501p1N6oNThub1oqDcWfZfWAlj4W1Zl8PHZDQ7L7VzUsA21U7OuqkbgUdaP/pcH4RrFu8JmC4zgpXCg4jpPChYLjOClcKDiOk6IbH42fp+W1eb+qvvSEvA8AHwdWqupz0nLY+GngUmAG+Jeq+kC0jVyB0mB4We+0oQwcGrU9BB05Yi8FLjTtfIBVy1Zmps+ovSS2onFlodSzl8YWu1B4DS+zlwsvWzVp5hc03EazbvddIu7ri/mwsrBQshWJ+ZJ9W+aL8WdZM5+tsJOIx9BqI64srEn2dW9qvGws5HExEiRWiuGxaxrX82SYb9g4RGQD8HrgmY7kS2h5cN4KXAN8ZuFddBynl8wrbFzCJ4HfIh3s5XLgRm1xDzAhImsWpaeO4/SE+cZ9uAzYpaoPnZA1r7Bxzx+ORFpyHKdnnLRQEJEh4EPA72RlZ6QFw8ap6gWqesHyCduDreM4vWM+Kxq3AJuAh5JAMOuBB0TkQjxsnOOc9pz0TEFVH1HVVaq6UVU30hIE56vqXlph466UFhcBR1R1z+J22XGcpWReYeNUNRQh6pu0zJE7aJkk39NNJxrNJoenwx5pRsbCnngOHzlk1r1sMjsoSJvKwefszgHHDmXvn6iW7IAoKnFPPJLLNkEVI8FWAIqB6MdtBsuRyNBimSRt210e23RWLofX4ZcHwntZko7Z+V1QD0Rv1shel8pMPMphJbD9oJn59nwCARN0GyvCOtj7G45Mz8Xb74L5ho2RTWVpAAAGKUlEQVTrzN/Y8VmB9y68W47jnCp8RaPjOClcKDiOk8KFguM4KfrCyQqSRwfCbrufORBe3PS1W24xqy5X7AjGL1lnO2kBGAystW/kIwozjQ9vIxAZulKNK7wKRVtpJYXIHoN8+JmgkedFLmcr1eqGorIe2JfQplSyFaTNpr1/AGAuoLiembFdrNciikCAw1PZdYTuk07KA/Y9ERvXmVr43PcdXJxFgD5TcBwnhQsFx3FSuFBwHCeFCwXHcVK4UHAcJ0VfWB+aCBUNa8q/dfffB/P+/oHHzbpff+ErzPyB4S52aNYCrtwDloM21XoXXnwCyvC5iJYcoDJr118djQRTNZZJF4u2ZaVWi2jpNWxhmJ6zzy2ft+vWRsRNOzA7cywzvVaxxywX8YwEkGtmn9vwQNzF+mDZtghVGrZHq0olPDbN/OL8nH2m4DhOChcKjuOkcKHgOE4KFwqO46RwoeA4TgoXCo7jpOgLk+RcpcoTT+8K5u9+Prypafch25PO237lKjP/vm9/3e4csDwQnKSQ7ZP2ONpFcI6mZpvAmo24aWy6bnvaqczZdRQOh8trxLylAY9RbfJGUBMrDyAXGJPjbTe78GjVyD6mGQn2ohGPUwCrV2V7Ahss2ucFQKBfx7OrseseNhWPTMY393WDzxQcx0nhQsFxnBQuFBzHSeFCwXGcFKJdREZe8k6IHACmgbi/9aVnBd6PTrwfaU7nfpylqtkh1DvoC6EAICL3qeoF3g/vh/fj1PbDXx8cx0nhQsFxnBT9JBQ+e6o7kOD9SOP9SPMPvh99o1NwHKc/6KeZguM4fYALBcdxUpxyoSAibxKR7SKyQ0Su62G7G0TkLhHZJiKPicj7kvSPiMguEXkw+bu0B315WkQeSdq7L0mbFJHbReRHyf9lS9yHczrO+UEROSoi7+/FeIjI50Vkv4g82pGWef7S4o+T++VhETl/ifvxcRF5ImnrayIykaRvFJHZjnG5fon7EbwOIvLBZDy2i8gbF9wBVT1lf0AeeBLYDJSAh4DzetT2GuD85PMo8EPgPOAjwAd6PA5PAytOSPuvwHXJ5+uAj/X4uuwFzurFeAA/D5wPPBo7f+BS4FuAABcB9y5xP94AFJLPH+vox8bO43owHpnXIblnH6K1fXJT8nvKL6T9Uz1TuBDYoapPqWoVuBm4vBcNq+oeVX0g+TwFbAPW9aLtLrkcuCH5fAPw1h62fTHwpKr+pBeNqep3gYMnJIfO/3LgRm1xDzAhImuWqh+q+h1Vbe93vgdYvxhtnWw/DC4HblbViqr+GNhB63c1b061UFgHPNvxfSen4IcpIhuBVwL3Jkm/lkwXP7/U0/YEBb4jIveLyDVJ2mpV3QMtAQas6kE/2lwBfKnje6/HA8LnfyrvmV+lNUtps0lEfiAifysir+lB+1nXYdHH41QLhSwvJD21kYrICPAV4P2qehT4DLAFeAWwB/ijHnTjZ1X1fOAS4L0i8vM9aDMTESkBlwFfTpJOxXhYnJJ7RkQ+BNSBm5KkPcCZqvpK4DeBvxSRLoKIzJvQdVj08TjVQmEnsKHj+3pgd68aF5EiLYFwk6p+FUBV96lqQ1WbwJ+xwKlYN6jq7uT/fuBrSZv72tPi5P/+pe5HwiXAA6q6L+lTz8cjIXT+Pb9nROQq4C3AuzR5kU+m688nn++n9S7/oqXqg3EdFn08TrVQ+D6wVUQ2JU+oK4DbetGwiAjwOWCbqn6iI73z/fRtwKMnll3kfgyLyGj7My3F1qO0xqHtS+4qIO43bnF4Jx2vDr0ejw5C538bcGVihbgIONJ+zVgKRORNwLXAZao605G+UkTyyefNwFbgqSXsR+g63AZcISIDIrIp6cf3FtTYUmhPT1LTeiktzf+TwId62O7P0ZpmPQw8mPxdCnwReCRJvw1Ys8T92ExLe/wQ8Fh7DIDlwB3Aj5L/kz0YkyHgeWC8I23Jx4OWENoD1Gg9+a4OnT+t6fKfJvfLI8AFS9yPHbTe2dv3yPXJsf88uV4PAQ8A/3SJ+xG8DsCHkvHYDlyy0PZ9mbPjOClO9euD4zh9hgsFx3FSuFBwHCeFCwXHcVK4UHAcJ4ULBcdxUrhQcBwnxf8DtvsNgtxbixIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for fun\n",
    "pyplot.imshow(random_face_pixels)\n",
    "title = '%s (%.3f)' % (predict_names[0], class_probability)\n",
    "pyplot.title(title)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
